{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text and converting pdf to images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Code: \n",
    "If we want to send the full folder contanng pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    \"\"\"Extracts images from PDF and saves them as PNGs.\"\"\"\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    pdf_output_folder = os.path.join(output_folder, pdf_name)\n",
    "    os.makedirs(pdf_output_folder, exist_ok=True)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        page = doc[i]\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        image_path = os.path.join(pdf_output_folder, f\"{pdf_name}_page_{i+1}.png\")\n",
    "        pix.save(image_path)\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def remove_background_noise(image):\n",
    "    \"\"\"Removes dots and unwanted background noise using morphological operations.\"\"\"\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    image = cv2.medianBlur(image, 3) \n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=1)  # Open to remove dots\n",
    "    return image\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    \"\"\"Applies CLAHE to improve contrast while preserving details.\"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def deskew_image(image):\n",
    "    \"\"\"Corrects skew in the document using Hough Line Transform.\"\"\"\n",
    "    edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "\n",
    "    if lines is None:\n",
    "        return image  # No deskew needed\n",
    "\n",
    "    angles = []\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        angle = np.arctan2(y2 - y1, x2 - x1) * (180 / np.pi)\n",
    "        angles.append(angle)\n",
    "\n",
    "    median_angle = np.median(angles)\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Applies noise removal, contrast enhancement, de-skewing, and adaptive thresholding.\"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    image = remove_background_noise(image)\n",
    "\n",
    "    image = enhance_contrast(image)\n",
    "\n",
    "    image = deskew_image(image)\n",
    "\n",
    "    image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                  cv2.THRESH_BINARY, 15, 5)\n",
    "\n",
    "    processed_path = image_path.replace(\".png\", \"_cleaned.png\")\n",
    "    cv2.imwrite(processed_path, image)\n",
    "\n",
    "    return processed_path\n",
    "\n",
    "def clean_pdf_folder(input_folder, output_folder):\n",
    "    \"\"\"Processes all PDFs in a given folder.\"\"\"\n",
    "    pdf_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    all_cleaned_images = []\n",
    "    for pdf_path in pdf_files:\n",
    "        image_paths = pdf_to_images(pdf_path, output_folder)\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            cleaned_images = list(executor.map(preprocess_image, image_paths))\n",
    "            all_cleaned_images.extend(cleaned_images)\n",
    "\n",
    "    return all_cleaned_images\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"D:\\OCR\\Pdfs\"  # Folder containing PDFs\n",
    "    output_folder = \"D:\\OCR\\Imgs\"  # Folder to save cleaned images\n",
    "    cleaned_files = clean_pdf_folder(input_folder, output_folder)\n",
    "    print(\"Cleaned images:\", cleaned_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Preprocessing code if we want to send pdf wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    \"\"\"Extracts images from PDF and saves them as PNGs.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        page = doc[i]\n",
    "        pix = page.get_pixmap(dpi=300)  # Increased DPI for better OCR accuracy\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        pix.save(image_path)\n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "def remove_background_noise(image):\n",
    "    \"\"\"Removes small dots and background noise using morphological operations.\"\"\"\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    image = cv2.medianBlur(image, 3)  # Reduce small noise\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=1)  # Open to remove dots\n",
    "    return image\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    \"\"\"Applies CLAHE (Contrast Limited Adaptive Histogram Equalization).\"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def deskew_image(image):\n",
    "    \"\"\"Corrects skew in the document using Hough Line Transform but avoids over-rotation.\"\"\"\n",
    "    edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "\n",
    "    if lines is None:\n",
    "        return image  # No rotation needed\n",
    "\n",
    "    angles = []\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        angle = np.arctan2(y2 - y1, x2 - x1) * (180 / np.pi)\n",
    "        angles.append(angle)\n",
    "\n",
    "    median_angle = np.median(angles)\n",
    "\n",
    "    # **Limit rotation angle to avoid excessive rotation**\n",
    "    if abs(median_angle) < 2 or abs(median_angle) > 10:\n",
    "        return image  # Ignore small angles or extreme rotations\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Enhances text visibility and removes background noise for OCR.\"\"\"\n",
    "    # Load image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Apply Non-Local Means Denoising (preserves text edges)\n",
    "    image = cv2.fastNlMeansDenoising(image, None, h=30, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "    # Adaptive Thresholding for better contrast\n",
    "    image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Morphological Opening (Removes small noise dots)\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Save and return cleaned image path\n",
    "    processed_path = image_path.replace(\".png\", \"_cleaned.png\")\n",
    "    cv2.imwrite(processed_path, image)\n",
    "    \n",
    "    return processed_path\n",
    "\n",
    "def clean_pdf(pdf_path, output_folder):\n",
    "    \"\"\"Cleans the PDF by converting pages to preprocessed images.\"\"\"\n",
    "    image_paths = pdf_to_images(pdf_path, output_folder)\n",
    "\n",
    "    # Use threading for efficient parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        cleaned_images = list(executor.map(preprocess_image, image_paths))\n",
    "\n",
    "    return cleaned_images\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"D:\\OCR\\OCR_fld\\Ezcaray - Vozes.pdf\"\n",
    "    output_folder = \"D:\\OCR\\Imgs\\Ezcaray\"\n",
    "    cleaned_files = clean_pdf(pdf_path, output_folder)\n",
    "    print(\"Cleaned images:\", cleaned_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tessaract to do OCR :\n",
    "To get a target text to compare our model with and also use it for fine Tuning the transformer..Just as a trial to see how tessaract performs on these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR completed! Text saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\DELL\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    \"\"\"Performs OCR on a cleaned image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "def ocr_on_cleaned_images(cleaned_folder):\n",
    "    \"\"\"Runs OCR on all cleaned images and saves text to a file.\"\"\"\n",
    "    text_output = []\n",
    "    \n",
    "    for file in sorted(os.listdir(cleaned_folder)):\n",
    "        #if file.endswith(\"_cleaned.png\"):\n",
    "        image_path = os.path.join(cleaned_folder, file)\n",
    "        extracted_text = ocr_image(image_path)\n",
    "        text_output.append(f\"Page {file}: \\n{extracted_text}\\n{'='*50}\")\n",
    "\n",
    "    with open(os.path.join(\"D:\\OCR\\Extracted_texts\", \"Benudia.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(text_output))\n",
    "\n",
    "    print(\"OCR completed! Text saved to the given path\")\n",
    "\n",
    "cleaned_folder = \"D:\\OCR\\PreprocessedImages\"\n",
    "ocr_on_cleaned_images(cleaned_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\DELL\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    \"\"\"Performs OCR on a cleaned image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "def ocr_on_cleaned_images(root_folder):\n",
    "    \"\"\"Runs OCR on all cleaned images within each subfolder and saves text to separate files.\"\"\"\n",
    "    for subfolder in sorted(os.listdir(root_folder)):\n",
    "        subfolder_path = os.path.join(root_folder, subfolder)\n",
    "        \n",
    "        if os.path.isdir(subfolder_path): \n",
    "            text_output = []\n",
    "            \n",
    "            for file in sorted(os.listdir(subfolder_path)):\n",
    "                if file.endswith(\"_cleaned.png\"): \n",
    "                    image_path = os.path.join(subfolder_path, file)\n",
    "                    extracted_text = ocr_image(image_path)\n",
    "                    text_output.append(f\"Page {file}: \\n{extracted_text}\\n{'='*50}\")\n",
    "            \n",
    "            output_file = os.path.join(root_folder, f\"{subfolder}.txt\")\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(text_output))\n",
    "            \n",
    "            print(f\"OCR completed for {subfolder}! Text saved to {output_file}\")\n",
    "\n",
    "cleaned_folder = \"D:\\\\OCR\\\\trialprep\"\n",
    "ocr_on_cleaned_images(cleaned_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers datasets pillow opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer based Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_17680\\3959084147.py:48: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  cleaned_folder = \"D:\\OCR\\Evalmgs\\Ezcaray\"\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Result for D:\\OCR\\Evalmgs\\Ezcaray\\page_1_cleaned.png:\n",
      "----\n",
      "J\n",
      "***\n",
      "CASHIER\n",
      "-:\n",
      "SENOR\n",
      "ILVSTRISSIMO.\n",
      "CASH\n",
      "CUPADO\n",
      "EN\n",
      "EL\n",
      "EXERCICIO\n",
      "9.00\n",
      "SR:\n",
      "DE LAS\n",
      "MITIONES.\n",
      "EN\n",
      "EL\n",
      "***\n",
      "OBILPADO\n",
      "DE\n",
      "GUADALA-\n",
      "AMOUNT\n",
      "XARA,REOIBI VNA\n",
      "AC\n",
      "V.S.I:\n",
      "AMOUNT\n",
      "AK\n",
      "EN QUE MO LA NOTICIA DE\n",
      "1\n",
      "NOTICIA DE\n",
      "COMO FU\n",
      "FU\n",
      "MAGGETTAD\n",
      "(\n",
      "QUE\n",
      "DIOS\n",
      "QUARDE)\n",
      "TE\n",
      "AVIA\n",
      "SERVIDO\n",
      "DE\n",
      "HONRARME\n",
      "CON\n",
      "1A\n",
      "MERCCO\n",
      "DE\n",
      "FU\n",
      "PREDICADOR;\n",
      "1\n",
      "COMONO\n",
      "TC\n",
      "OPONE\n",
      "1A\n",
      "PREDICACION\n",
      "DE\n",
      "FU\n",
      "MAGER\n",
      "TAD\n",
      "ALA\n",
      "APOTOLICA,TUVE\n",
      "PORDE\n",
      "MI OBLI\n",
      "GACION ADMITR EL FAVOR\n",
      ":\n",
      "FINDIEDO\n",
      "2\n",
      "V.S.I.EL\n",
      "AGRADECIMIENTO.\n",
      "EI\n",
      "RCY\n",
      "MI\n",
      "FEROR(\n",
      "QUE\n",
      "DIOS\n",
      "GUARDE)\n",
      "HIZO\n",
      "IA GRACIA ;\n",
      "MAS\n",
      "A.V.S.I.\n",
      "TE TE\n",
      "ACBE:\n",
      "QUC\n",
      "POP\n",
      "MAS.\n",
      "FRUTOS, QUE DIERA\n",
      "1A\n",
      "TIERRA\n",
      "DE\n",
      "PROMITION\n",
      ">\n",
      "NO\n",
      "IOS\n",
      "LORARA\n",
      "TOYICS;\n",
      "G.JOFUE,Y\n",
      "CALCB\n",
      "MO\n",
      "IOS\n",
      "FACEIN:\n",
      ":DOS\n",
      "TACARON\n",
      "EL\n",
      "FRUTO,\n",
      "Y\n",
      "DE\n",
      "AMBOS\n",
      "RECEFITO;\n",
      "PARA\n",
      "HALLAR,VN\n",
      "GIMIL\n",
      "PROPORCIONADO\n",
      "AL'A\n",
      "GRANDCZA\n",
      "DE\n",
      "V.S.I.\n",
      "***\n",
      "*\n",
      "* 2\n",
      "A\n",
      "\n",
      "OCR completed. Results saved to D:\\OCR\\Evalmgs\\Ezcaray\\ocr_results.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\DELL\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "# I have used tesssaract to mark the boxes in this case.\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def get_text_regions(image_path):\n",
    "    \"\"\"Detects text regions using Tesseract and returns bounding boxes.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    d = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "    boxes = []\n",
    "    for i in range(len(d[\"text\"])):\n",
    "        if d[\"text\"][i].strip(): \n",
    "            (x, y, w, h) = (d[\"left\"][i], d[\"top\"][i], d[\"width\"][i], d[\"height\"][i])\n",
    "            boxes.append((x, y, x + w, y + h))\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def perform_ocr(image_path):\n",
    "    \"\"\"Performs OCR using TrOCR on detected text regions.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    boxes = get_text_regions(image_path)\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for box in boxes:\n",
    "        cropped = img.crop(box)\n",
    "        cropped = cropped.resize((384, 384))\n",
    "        \n",
    "        pixel_values = processor(images=cropped, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(pixel_values, max_length=128)\n",
    "\n",
    "        extracted_text += processor.batch_decode(output_ids, skip_special_tokens=True)[0] + \"\\n\"\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "cleaned_folder = \"D:\\OCR\\Evalmgs\\Ezcaray\"\n",
    "cleaned_images = [os.path.join(cleaned_folder, f) for f in os.listdir(cleaned_folder) if f.endswith(\"_cleaned.png\")]\n",
    "\n",
    "ocr_results = {}\n",
    "for image_path in cleaned_images:\n",
    "    extracted_text = perform_ocr(image_path)\n",
    "    ocr_results[image_path] = extracted_text\n",
    "    print(f\"OCR Result for {image_path}:\\n{extracted_text}\\n\")\n",
    "\n",
    "output_txt = os.path.join(cleaned_folder, \"ocr_results.txt\")\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    for img, text in ocr_results.items():\n",
    "        f.write(f\"{img}:\\n{text}\\n\\n\")\n",
    "\n",
    "print(f\"OCR completed. Results saved to {output_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download es_core_news_sm --> Run this in the terminal to install it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy to correct few spellings:\n",
    "As we can see the outputs of the pretrained transformer model are boken and mostly wordwise. So, I am using Spacy to coorect it and give the text output in a formatted way as that is the ultimate requirement. We need to compare how this pretrained transformer model works wrt PyTessaract Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR text structured and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def correct_text(text):\n",
    "    \"\"\"Tokenizes and corrects OCR text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc])\n",
    "\n",
    "def words_to_sentences(text):\n",
    "    \"\"\"Converts word-by-word OCR output into structured sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "input_txt_file = \"D:\\\\OCR\\\\Evalmgs\\\\Ezcaray\\\\ocr_results.txt\"\n",
    "output_txt_file = \"D:\\\\OCR\\\\Evalmgs\\\\Ezcaray\\\\res.txt\"\n",
    "\n",
    "with open(input_txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    ocr_text = f.readlines() \n",
    "\n",
    "structured_output = []\n",
    "current_page = None\n",
    "current_text = []\n",
    "\n",
    "for line in ocr_text:\n",
    "    line = line.strip()\n",
    "    \n",
    "    if re.match(r\"D:\\\\OCR\\\\Evalmgs\\\\Ezcaray\\\\page_\\d+_cleaned.png:\", line):\n",
    "        if current_page and current_text:\n",
    "            corrected_text = correct_text(\" \".join(current_text))\n",
    "            structured_text = words_to_sentences(corrected_text)\n",
    "            structured_output.append(f\"{current_page}\\n{structured_text}\\n\")\n",
    "        \n",
    "        current_page = line\n",
    "        current_text = []\n",
    "    \n",
    "    else:\n",
    "        current_text.append(line)\n",
    "\n",
    "if current_page and current_text:\n",
    "    corrected_text = correct_text(\" \".join(current_text))\n",
    "    structured_text = words_to_sentences(corrected_text)\n",
    "    structured_output.append(f\"ðŸ“„ {current_page}\\n{structured_text}\\n\")\n",
    "\n",
    "with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(structured_output))\n",
    "\n",
    "print(\"OCR text structured and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "with open(\"D:\\\\OCR\\\\Evalmgs\\\\Ezcaray\\\\res.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text1 = f.read()\n",
    "\n",
    "with open(\"D:\\\\OCR\\\\textfiles\\\\Ezcaray\\\\page_1_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text2 = f.read()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "print(f\"Cosine Similarity: {similarity_score[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-based Similarity: 0.5872\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with open(\"D:\\\\OCR\\\\Evalmgs\\\\Ezcaray\\\\res.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text1 = f.read()\n",
    "\n",
    "with open(\"D:\\\\OCR\\\\textfiles\\\\Ezcaray\\\\page_1_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text2 = f.read()\n",
    "\n",
    "embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "similarity = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(f\"BERT-based Similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fine Tuned Transformers and CRNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "# Define the CRNN Model\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256, num_lstm_layers=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # Feature extractor (ResNet without fully connected layer)\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # Remove FC layer\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # LSTM for sequence modeling\n",
    "        self.rnn = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=num_lstm_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # BiLSTM doubles output size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)  # Extract visual features (B, C, H, W)\n",
    "        features = features.squeeze(2) if features.shape[2] == 1 else features  # Ensure H=1\n",
    "        print(features.shape)\n",
    "        features = features.permute(0, 2, 1)  # (B, W, C)\n",
    "        features = features.squeeze(2).permute(0, 2, 1)  # (B, W, C)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(features)  # BiLSTM output (B, W, 2*hidden_size)\n",
    "        output = self.fc(rnn_out)  # Convert to character logits\n",
    "        return output\n",
    "\n",
    "# OCR Dataset\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Load data function\n",
    "def load_data(image_root, text_root):\n",
    "    paths, labels = [], []\n",
    "    for folder in os.listdir(image_root):\n",
    "        img_dir = os.path.join(image_root, folder)\n",
    "        txt_dir = os.path.join(text_root, folder)\n",
    "        \n",
    "        if not os.path.isdir(img_dir) or not os.path.isdir(txt_dir):\n",
    "            continue\n",
    "            \n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if img_file.endswith(\".png\"):\n",
    "                base = img_file[:-4]\n",
    "                txt_path = os.path.join(txt_dir, f\"{base}.txt\")\n",
    "                if os.path.exists(txt_path):\n",
    "                    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        paths.append(os.path.join(img_dir, img_file))\n",
    "                        labels.append(f.read().strip())\n",
    "    return paths, labels\n",
    "\n",
    "# Data transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_paths, train_labels = load_data(\"D:/OCR/Imgs\", \"D:/OCR/textfiles\")\n",
    "eval_paths, eval_labels = load_data(\"D:/OCR/Evalmgs\", \"D:/OCR/EvalText\")\n",
    "\n",
    "train_dataset = OCRDataset(train_paths, train_labels, transform)\n",
    "eval_dataset = OCRDataset(eval_paths, eval_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "num_classes = 128  # Adjust based on vocabulary size\n",
    "model = CRNN(num_classes=num_classes).to(device)\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            target_lengths = torch.IntTensor([len(label) for label in labels])\n",
    "            target = torch.cat([torch.IntTensor([ord(c) for c in label]) for label in labels])\n",
    "            \n",
    "            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.int32)\n",
    "            loss = criterion(outputs.permute(1, 0, 2), target, input_lengths, target_lengths)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "torch.save(model.state_dict(), \"crnn_ocr.pth\")\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuned Transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code for Fine Tuning the Pre Trained Transformer Model: \n",
    "Needs more Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 8.9213\n",
      "Epoch 1, Validation CER: 0.9770\n",
      "Epoch 2, Training Loss: 7.4886\n",
      "Epoch 2, Validation CER: 0.9794\n",
      "Epoch 3, Training Loss: 7.1206\n",
      "Epoch 3, Validation CER: 1.0000\n",
      "Epoch 4, Training Loss: 7.0006\n",
      "Epoch 4, Validation CER: 0.9790\n",
      "Epoch 5, Training Loss: 6.9628\n",
      "Epoch 5, Validation CER: 0.9955\n",
      "Fine-tuning complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "# Ensure pad_token_id and decoder_start_token_id are set\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "# Fix decoder_start_token_id issue\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id  # <-- ADD THIS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except the last few in the decoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.decoder.model.decoder.layers[-5:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Custom Dataset Class\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, img_folder, text_folder, processor):\n",
    "        self.img_paths = []\n",
    "        self.texts = []\n",
    "        self.processor = processor\n",
    "        \n",
    "        for subdir in os.listdir(img_folder):\n",
    "            img_subfolder = os.path.join(img_folder, subdir)\n",
    "            text_subfolder = os.path.join(text_folder, subdir)\n",
    "            if not os.path.isdir(img_subfolder):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(img_subfolder):\n",
    "                if img_name.endswith(\"_cleaned.png\"):\n",
    "                    img_path = os.path.join(img_subfolder, img_name)\n",
    "                    text_path = os.path.join(text_subfolder, img_name.replace(\".png\", \".txt\"))\n",
    "                    \n",
    "                    if os.path.exists(text_path):\n",
    "                        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            text = f.read().strip()\n",
    "                        self.img_paths.append(img_path)\n",
    "                        self.texts.append(text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        labels = self.processor.tokenizer(self.texts[idx], return_tensors=\"pt\", max_length=512, truncation=True).input_ids.squeeze(0)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels_padded}\n",
    "\n",
    "train_dataset = OCRDataset(\"D:\\\\OCR\\\\Imgs\", \"D:\\\\OCR\\\\textfiles\", processor)\n",
    "val_dataset = OCRDataset(\"D:\\\\OCR\\\\Evalmgs\", \"D:\\\\OCR\\\\EvalText\", processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_cer = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            output_ids = model.generate(pixel_values)\n",
    "            predictions = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            references = [processor.tokenizer.decode(lbl[lbl!=-100], skip_special_tokens=True) for lbl in labels]\n",
    "            \n",
    "            total_cer += metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    avg_cer = total_cer / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation CER: {avg_cer:.4f}\")\n",
    "\n",
    "model.save_pretrained(\"D:\\\\OCR\\\\fine_tuned_trocr1\")\n",
    "processor.save_pretrained(\"D:\\\\OCR\\\\fine_tuned_trocr1\")\n",
    "print(\"Fine-tuning complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"qantev/trocr-large-spanish\", do_rescale=False)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"qantev/trocr-large-spanish\")\n",
    "\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except the last few in the decoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.decoder.model.decoder.layers[-5:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Custom Dataset Class\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, img_folder, text_folder, processor):\n",
    "        self.img_paths = []\n",
    "        self.texts = []\n",
    "        self.processor = processor\n",
    "        \n",
    "        for subdir in os.listdir(img_folder):\n",
    "            img_subfolder = os.path.join(img_folder, subdir)\n",
    "            text_subfolder = os.path.join(text_folder, subdir)\n",
    "            if not os.path.isdir(img_subfolder):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(img_subfolder):\n",
    "                if img_name.endswith(\"_cleaned.png\"):\n",
    "                    img_path = os.path.join(img_subfolder, img_name)\n",
    "                    text_path = os.path.join(text_subfolder, img_name.replace(\".png\", \".txt\"))\n",
    "                    \n",
    "                    if os.path.exists(text_path):\n",
    "                        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            text = f.read().strip()\n",
    "                        self.img_paths.append(img_path)\n",
    "                        self.texts.append(text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        labels = self.processor.tokenizer(self.texts[idx], return_tensors=\"pt\", max_length=512, truncation=True).input_ids.squeeze(0)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels_padded}\n",
    "\n",
    "train_dataset = OCRDataset(\"D:\\\\OCR\\\\Imgs\", \"D:\\\\OCR\\\\textfiles\", processor)\n",
    "val_dataset = OCRDataset(\"D:\\\\OCR\\\\Evalmgs\", \"D:\\\\OCR\\\\EvalText\", processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_cer = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            output_ids = model.generate(pixel_values)\n",
    "            predictions = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            references = [processor.tokenizer.decode(lbl[lbl!=-100], skip_special_tokens=True) for lbl in labels]\n",
    "            \n",
    "            total_cer += metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    avg_cer = total_cer / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation CER: {avg_cer:.4f}\")\n",
    "\n",
    "model.save_pretrained(\"D:\\\\OCR\\\\fine_tuned_trocr2\")\n",
    "processor.save_pretrained(\"D:\\\\OCR\\\\fine_tuned_trocr2\")\n",
    "print(\"Fine-tuning complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code for Fine Tuned Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load fine-tuned model and processor\n",
    "model_path = \"D:\\\\OCR\\\\fine_tuned_trocr\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_path)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# OCR Function\n",
    "def perform_ocr(img_folder, output_file):\n",
    "    results = []\n",
    "    \n",
    "    for img_name in os.listdir(img_folder):\n",
    "        if img_name.endswith(\".png\") or img_name.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Process image\n",
    "            pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            \n",
    "            # Generate text\n",
    "            output_ids = model.generate(pixel_values)\n",
    "            text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            results.append(f\"{img_name}: {text}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(results))\n",
    "    \n",
    "    print(f\"OCR completed. Results saved to {output_file}\")\n",
    "\n",
    "# Example Usage\n",
    "img_folder = \"D:\\OCR\\Evalmgs\\Ezcaray\"\n",
    "output_file = \"D:\\\\OCR\\\\output1.txt\"\n",
    "perform_ocr(img_folder, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
